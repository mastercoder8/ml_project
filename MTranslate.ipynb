{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.models as word2vec\n",
    "model_en = word2vec.Word2Vec.load('englishwords')\n",
    "model_fr = word2vec.Word2Vec.load('frenchwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines_en = open('test.en','r',encoding='utf8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_en = open('test.en','r',encoding='utf8').read()\n",
    "data_en = data_en.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english file has 252583 words,19176 unique\n"
     ]
    }
   ],
   "source": [
    "words_en = list(set(data_en))\n",
    "data_size_en,vocab_size_en = len(data_en),len(words_en)\n",
    "print('english file has %d words,%d unique'%(data_size_en,vocab_size_en))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec English unique words = 19176\n"
     ]
    }
   ],
   "source": [
    "# word_to_ix_en = { w:model.wv[w] for w in (words_en)}\n",
    "#ix_to_word_en = { model.wv[w]:w for w in (words_en)}\n",
    "\n",
    "for w in words_en:\n",
    "    word_to_ix_en[w] = model_en.wv[w]\n",
    "    ix_to_word_en[tuple(model_en.wv[w])] = w\n",
    "print('Word2Vec English unique words =',len(word_to_ix_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "french file has 291815 words,21084 unique\n"
     ]
    }
   ],
   "source": [
    "lines_fr = open('test.fr','r',encoding='utf8').readlines()\n",
    "data_fr = open('test.fr','r',encoding='utf8').read()\n",
    "data_fr = data_fr.split()\n",
    "words_fr = list(set(data_fr))\n",
    "data_size_fr,vocab_size_fr = len(data_fr),len(words_fr)\n",
    "print('french file has %d words,%d unique'%(data_size_fr,vocab_size_fr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec French unique words = 21084\n"
     ]
    }
   ],
   "source": [
    "#word_to_ix_fr = { w:i for i,w in enumerate(words_fr)}\n",
    "#ix_to_word_fr = { i:w for i,w in enumerate(words_fr)}\n",
    "\n",
    "for w in words_fr:\n",
    "    word_to_ix_fr[w] = model_fr.wv[w]\n",
    "    ix_to_word_fr[tuple(model_fr.wv[w])] = w\n",
    "print('Word2Vec French unique words =',len(word_to_ix_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num = len(lines_en)\n",
    "num = len(lines_fr) #No of Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters which are same for both encoder and decoder\n",
    "hidden_size = 100\n",
    "learning_rate = 1e-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoder weight parameters for english language\n",
    "wxh_en = np.random.randn(hidden_size,vocab_size_en)*0.01\n",
    "whh_en = np.random.randn(hidden_size,hidden_size)*0.01\n",
    "why_en = np.random.randn(vocab_size_en,hidden_size)*0.01\n",
    "bh_en = np.zeros((hidden_size,1))\n",
    "by_en = np.zeros((vocab_size_en,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#decoder weight parameters for french language\n",
    "wxh_fr = np.random.randn(hidden_size,vocab_size_fr)*0.01\n",
    "whh_fr = np.random.randn(hidden_size,hidden_size)*0.01\n",
    "why_fr = np.random.randn(vocab_size_fr,hidden_size)*0.01\n",
    "bh_fr = np.zeros((hidden_size,1))\n",
    "by_fr = np.zeros((vocab_size_fr,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainencoder(inputs, targets, hprev):\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size_en,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(wxh_en, xs[t]) + np.dot(whh_en, hs[t-1]) + bh_en) # hidden state\n",
    "    ys[t] = np.dot(why_en, hs[t]) + by_en # unnormalized log probabilities for next words\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next words\n",
    "\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(wxh_en), np.zeros_like(whh_en), np.zeros_like(why_en)\n",
    "  dbh, dby = np.zeros_like(bh_en), np.zeros_like(by_en)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(why_en.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(whh_en.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traindecoder(inputs, targets, hprev):\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size_fr,1)) # encode in 1-of-k representation\n",
    "    if(inputs[t]!=-1):\n",
    "        xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(wxh_fr, xs[t]) + np.dot(whh_fr, hs[t-1]) + bh_fr) # hidden state\n",
    "    ys[t] = np.dot(why_fr, hs[t]) + by_fr # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(wxh_fr), np.zeros_like(whh_fr), np.zeros_like(why_fr)\n",
    "  dbh, dby = np.zeros_like(bh_fr), np.zeros_like(by_fr)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(why_fr.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(whh_fr.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(inputs, targets):\n",
    "  xs, hs, ys = {}, {}, {}\n",
    "  hs[-1] = np.zeros((hidden_size,1))\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size_en,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(wxh_en, xs[t]) + np.dot(whh_en, hs[t-1]) + bh_en) # hidden state\n",
    "    ys[t] = np.dot(why_en, hs[t]) + by_en \n",
    "  hprev = hs[len(inputs)-1]\n",
    "  tem = \"\"\n",
    "  ans = \"\"\n",
    "  k = 0\n",
    "  t = -1\n",
    "  while k<10:\n",
    "      x = np.zeros((vocab_size_fr,1))\n",
    "      if(t!=-1):\n",
    "          x[t] = 1\n",
    "      hprev = np.tanh(np.dot(wxh_fr, x) + np.dot(whh_fr, hprev) + bh_fr)\n",
    "      y = np.dot(why_fr, hprev) + by_fr # unnormalized log probabilities for next chars\n",
    "      pr = np.exp(y) / np.sum(np.exp(y))\n",
    "      maxi = pr[0][0]\n",
    "      for i in range(len(words_fr)):\n",
    "          if pr[i][0]>=maxi:\n",
    "              maxi = pr[i][0]\n",
    "              t = i\n",
    "      tem = ix_to_word_fr[t]\n",
    "      k = k + 1\n",
    "      ans = ans + \" \" +tem\n",
    "  return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n"
     ]
    }
   ],
   "source": [
    "n,p = 1,0\n",
    "mwxh_en,mwhh_en,mwhy_en,mbh_en,mby_en = np.zeros_like(wxh_en),np.zeros_like(whh_en),np.zeros_like(why_en),np.zeros_like(bh_en),np.zeros_like(by_en)\n",
    "mwxh_fr,mwhh_fr,mwhy_fr,mbh_fr,mby_fr = np.zeros_like(wxh_fr),np.zeros_like(whh_fr),np.zeros_like(why_fr),np.zeros_like(bh_fr),np.zeros_like(by_fr)\n",
    "print(n,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\bTraining Encoder 1 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-ecb2fce866f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mhprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\bTraining Encoder\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdwxh_en\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdwhh_en\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdwhy_en\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdbh_en\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdby_en\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_en\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets_en\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\bTraining Decoder\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdwxh_fr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdwhh_fr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdwhy_fr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdbh_fr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdby_fr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraindecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_fr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets_fr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-419dbd90343e>\u001b[0m in \u001b[0;36mtrainencoder\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size_en\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# encode in 1-of-k representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwxh_en\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhh_en\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbh_en\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# hidden state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhy_en\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mby_en\u001b[0m \u001b[1;31m# unnormalized log probabilities for next words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "while n!=100:\n",
    "    curr_en = lines_en[p].split()\n",
    "    inputs_en = [word_to_ix_en[w] for w in curr_en[0:len(curr_en)-1]]\n",
    "    targets_en = [word_to_ix_en[w] for w in curr_en[1:len(curr_en)]]\n",
    "    \n",
    "    curr_fr = lines_fr[p].split()\n",
    "    inputs_fr=[-1]\n",
    "    temp = [word_to_ix_fr[w] for w in curr_fr[0:len(curr_fr)-1]]\n",
    "    inputs_fr.extend(temp)\n",
    "    targets_fr = [word_to_ix_fr[w] for w in curr_fr[0:len(curr_fr)]]\n",
    "\n",
    "    hprev = np.zeros((hidden_size,1))\n",
    "    print(\"\\bTraining Encoder\",n,p)\n",
    "    dwxh_en,dwhh_en,dwhy_en,dbh_en,dby_en,hprev = trainencoder(inputs_en,targets_en,hprev)\n",
    "    print(\"\\bTraining Decoder\",n,p)\n",
    "    dwxh_fr,dwhh_fr,dwhy_fr,dbh_fr,dby_fr,hprev = traindecoder(inputs_fr,targets_fr,hprev)\n",
    "    p += 1 # move sentence pointer\n",
    "    if p >= num:\n",
    "        p = 0\n",
    "        print('training...iteration:%d'%(n))\n",
    "        input_english = \"A note on the outcome of the discussions at the retreat will be presented separately.\"\n",
    "        curr_en = input_english.split()\n",
    "        inputs_en = [word_to_ix_en[ch] for ch in curr_en[0:len(curr_en)-1]]\n",
    "        targets_en = [word_to_ix_en[ch] for ch in curr_en[1:len(curr_en)]]\n",
    "        output_words = test(inputs_en,targets_en)\n",
    "        print(output_words)  \n",
    "        n = n + 1\n",
    "    for param_en, dparam_en, mem_en in zip([wxh_en, whh_en, why_en, bh_en, by_en], \n",
    "                                [dwxh_en, dwhh_en, dwhy_en, dbh_en, dby_en], \n",
    "                                [mwxh_en, mwhh_en, mwhy_en, mbh_en, mby_en]):\n",
    "      mem_en += dparam_en * dparam_en\n",
    "      param_en += -learning_rate * dparam_en / np.sqrt(mem_en + 1e-8) # adagrad update\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for param_fr, dparam_fr, mem_fr in zip([wxh_fr, whh_fr, why_fr, bh_fr, by_fr], \n",
    "                                [dwxh_fr, dwhh_fr, dwhy_fr, dbh_fr, dby_fr], \n",
    "                                [mwxh_fr, mwhh_fr, mwhy_fr, mbh_fr, mby_fr]):\n",
    "      mem_fr += dparam_fr * dparam_fr\n",
    "      param_fr += -learning_rate * dparam_fr / np.sqrt(mem_fr + 1e-8) # adagrad update\n",
    " # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
