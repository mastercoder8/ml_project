{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jcyar\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim.models as word2vec\n",
    "model_en = word2vec.Word2Vec.load('englishwords')\n",
    "model_fr = word2vec.Word2Vec.load('frenchwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lines_en = open('test.en','r',encoding='utf8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_en = open('test.en','r',encoding='utf8').read()\n",
    "data_en = data_en.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english file has 252583 words,300 features\n"
     ]
    }
   ],
   "source": [
    "words_en = list(set(data_en))\n",
    "data_size_en,vocab_size_en = len(data_en),len(model_en.wv['the'])\n",
    "print('english file has %d words,%d features'%(data_size_en,vocab_size_en))\n",
    "\n",
    "word_to_ix_en = {}\n",
    "ix_to_word_en = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec English unique words = 19176, word2vector features = 300\n"
     ]
    }
   ],
   "source": [
    "for w in words_en:\n",
    "    word_to_ix_en[w] = model_en.wv[w]\n",
    "    ix_to_word_en[tuple(model_en.wv[w])] = w\n",
    "print('Word2Vec English unique words = %d, word2vector features = %d'%(len(word_to_ix_en),len(model_en.wv['the'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "french file has 291815 words,300 features\n"
     ]
    }
   ],
   "source": [
    "lines_fr = open('test.fr','r',encoding='utf8').readlines()\n",
    "data_fr = open('test.fr','r',encoding='utf8').read()\n",
    "data_fr = data_fr.split()\n",
    "words_fr = list(set(data_fr))\n",
    "data_size_fr,vocab_size_fr = len(data_fr),len(model_en.wv['la'])\n",
    "print('french file has %d words,%d features'%(data_size_fr,vocab_size_fr))\n",
    "word_to_ix_fr = {}\n",
    "ix_to_word_fr = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec French unique words = 21084\n"
     ]
    }
   ],
   "source": [
    "for w in words_fr:\n",
    "    word_to_ix_fr[w] = model_fr.wv[w]\n",
    "    ix_to_word_fr[tuple(model_fr.wv[w])] = w\n",
    "print('Word2Vec French unique words =',len(word_to_ix_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num = len(lines_en)\n",
    "num = len(lines_fr) #No of Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters which are same for both encoder and decoder\n",
    "hidden_size = 100\n",
    "learning_rate = 1e-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#encoder weight parameters for english language\n",
    "wxh_en = np.random.randn(hidden_size,vocab_size_en)*0.01\n",
    "whh_en = np.random.randn(hidden_size,hidden_size)*0.01\n",
    "why_en = np.random.randn(vocab_size_en,hidden_size)*0.01\n",
    "bh_en = np.zeros((hidden_size,1))\n",
    "by_en = np.zeros((vocab_size_en,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#decoder weight parameters for french language\n",
    "wxh_fr = np.random.randn(hidden_size,vocab_size_fr)*0.01\n",
    "whh_fr = np.random.randn(hidden_size,hidden_size)*0.01\n",
    "why_fr = np.random.randn(vocab_size_fr,hidden_size)*0.01\n",
    "bh_fr = np.zeros((hidden_size,1))\n",
    "by_fr = np.zeros((vocab_size_fr,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def trainencoder(inputs, targets, hprev):\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  # forward pass  \n",
    "#   print(len(inputs))\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size_en,1)) # encode in 1-of-k representation\n",
    "    a = np.reshape(inputs[t],(300,1))\n",
    "    xs[t] = np.copy(a)\n",
    "    hs[t] = np.tanh(np.dot(wxh_en, xs[t]) + np.dot(whh_en, hs[t-1]) + bh_en) # hidden state\n",
    "    ys[t] = np.dot(why_en, hs[t]) + by_en # unnormalized log probabilities for next words\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next words\n",
    "#   print(hs)\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(wxh_en), np.zeros_like(whh_en), np.zeros_like(why_en)\n",
    "  dbh, dby = np.zeros_like(bh_en), np.zeros_like(by_en)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    target = np.reshape(targets[t],(300,1))\n",
    "    dy = np.copy(ys[t])\n",
    "    dy -= target  # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(why_en.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(whh_en.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def traindecoder(inputs, targets, hprev):\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size_fr,1)) # encode in 1-of-k representation\n",
    "    if(int(inputs[0])!=-1):\n",
    "        a = np.reshape(inputs[t],(300,1))\n",
    "        xs[t] = np.copy(a)\n",
    "    hs[t] = np.tanh(np.dot(wxh_fr, xs[t]) + np.dot(whh_fr, hs[t-1]) + bh_fr) # hidden state\n",
    "    ys[t] = np.dot(why_fr, hs[t]) + by_fr # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(wxh_fr), np.zeros_like(whh_fr), np.zeros_like(why_fr)\n",
    "  dbh, dby = np.zeros_like(bh_fr), np.zeros_like(by_fr)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    target = np.reshape(targets[t],(300,1))\n",
    "    dy = np.copy(ys[t])\n",
    "    dy -= target # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(why_fr.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(whh_fr.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test(inputs, targets):\n",
    "  xs, hs, ys = {}, {}, {}\n",
    "  hs[-1] = np.zeros((hidden_size,1))\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    a = np.reshape(inputs[t],(300,1))\n",
    "    xs[t] = np.zeros((vocab_size_en,1)) # encode in 1-of-k representation\n",
    "    xs[t] = np.copy(a)\n",
    "    hs[t] = np.tanh(np.dot(wxh_en, xs[t]) + np.dot(whh_en, hs[t-1]) + bh_en) # hidden state\n",
    "    ys[t] = np.dot(why_en, hs[t]) + by_en \n",
    "  hprev = hs[len(inputs)-1]\n",
    "  tem = \"\"\n",
    "  ans = \"\"\n",
    "  k = 0\n",
    "  t = -1\n",
    "  while k<10:\n",
    "      x = np.zeros((vocab_size_fr,1))\n",
    "      if(t!=-1):\n",
    "          x[t] = 1\n",
    "      hprev = np.tanh(np.dot(wxh_fr, x) + np.dot(whh_fr, hprev) + bh_fr)\n",
    "      y = np.dot(why_fr, hprev) + by_fr # unnormalized log probabilities for next chars\n",
    "      keys = np.zeros((len(ix_to_word_fr)))\n",
    "      mind = 100000.0\n",
    "      min_key = tuple\n",
    "      for ix in ix_to_word_fr.keys():\n",
    "          cosdist = np.dot(np.asarray(ix).T,y)/(np.linalg.norm(np.asarray(ix))*np.linalg.norm(y))\n",
    "          if(cosdist<mind):\n",
    "                mind = cosdist\n",
    "                minkey = ix\n",
    "#       print(keys.shape,y.shape)\n",
    "#       print(ix_to_word_fr[minkey],mind)\n",
    "      tem = ix_to_word_fr[minkey]\n",
    "#       pr = np.exp(y) / np.sum(np.exp(y))\n",
    "#       print(y.shape,y.reshape(300).shape)\n",
    "#       maxi = y[0][0]\n",
    "#       for i in range(len(words_fr)):\n",
    "#           if y[i][0]>=maxi:\n",
    "#               maxi = y[i][0]\n",
    "#               t = i\n",
    "#       arr = np.asarray(ix_to_word_fr);\n",
    "#       print(arr[np.linalg.norm(arr-y, axis=1).argmin()]) # closest key to y vector is the word\n",
    "      #tem = ix_to_word_fr[tuple(y.reshape(300))] # J Put word in it. find the closest word to y(vector)\n",
    "      k = k + 1\n",
    "      ans = ans + \" \" +tem\n",
    "  return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n,p = 1,0\n",
    "mwxh_en,mwhh_en,mwhy_en,mbh_en,mby_en = np.zeros_like(wxh_en),np.zeros_like(whh_en),np.zeros_like(why_en),np.zeros_like(bh_en),np.zeros_like(by_en)\n",
    "mwxh_fr,mwhh_fr,mwhy_fr,mbh_fr,mby_fr = np.zeros_like(wxh_fr),np.zeros_like(whh_fr),np.zeros_like(why_fr),np.zeros_like(bh_fr),np.zeros_like(by_fr)\n",
    "print(n,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "while n!=2000:\n",
    "    curr_en = lines_en[p].split()\n",
    "    inputs_en = [word_to_ix_en[w] for w in curr_en[0:len(curr_en)-1]]\n",
    "    targets_en = [word_to_ix_en[w] for w in curr_en[1:len(curr_en)]]\n",
    "    curr_fr = lines_fr[p].split()\n",
    "    inputs_fr=[-1]\n",
    "    temp = [word_to_ix_fr[w] for w in curr_fr[0:len(curr_fr)-1]]\n",
    "    inputs_fr.extend(temp)\n",
    "    targets_fr = [word_to_ix_fr[w] for w in curr_fr[0:len(curr_fr)]]\n",
    "    if(len(inputs_en)==0):\n",
    "        p = p+1\n",
    "        continue\n",
    "    hprev = np.zeros((hidden_size,1))\n",
    "#     print(\"\\bTraining Encoder\",n,p)\n",
    "    dwxh_en,dwhh_en,dwhy_en,dbh_en,dby_en,hprev = trainencoder(inputs_en,targets_en,hprev)\n",
    "#     print(\"\\bTraining Decoder\",n,p)\n",
    "    dwxh_fr,dwhh_fr,dwhy_fr,dbh_fr,dby_fr,hprev = traindecoder(inputs_fr,targets_fr,hprev)\n",
    "    p += 1 # move sentence pointer\n",
    "    if p >= num:\n",
    "        p = 0\n",
    "        print('\\b>>Training...iteration:%d'%(n))\n",
    "        input_english = \"in that context.\"\n",
    "        curr_en = input_english.split()\n",
    "        inputs_en = [word_to_ix_en[w] for w in curr_en[0:len(curr_en)-1]]\n",
    "        targets_en = [word_to_ix_en[w] for w in curr_en[1:len(curr_en)]]\n",
    "        output_words = test(inputs_en,targets_en)\n",
    "        print(output_words)  \n",
    "        n = n + 1\n",
    "    for param_en, dparam_en, mem_en in zip([wxh_en, whh_en, why_en, bh_en, by_en], \n",
    "                                [dwxh_en, dwhh_en, dwhy_en, dbh_en, dby_en], \n",
    "                                [mwxh_en, mwhh_en, mwhy_en, mbh_en, mby_en]):\n",
    "      mem_en += dparam_en * dparam_en\n",
    "      param_en += -learning_rate * dparam_en / np.sqrt(mem_en + 1e-8) # adagrad update\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for param_fr, dparam_fr, mem_fr in zip([wxh_fr, whh_fr, why_fr, bh_fr, by_fr], \n",
    "                                [dwxh_fr, dwhh_fr, dwhy_fr, dbh_fr, dby_fr], \n",
    "                                [mwxh_fr, mwhh_fr, mwhy_fr, mbh_fr, mby_fr]):\n",
    "      mem_fr += dparam_fr * dparam_fr\n",
    "      param_fr += -learning_rate * dparam_fr / np.sqrt(mem_fr + 1e-8) # adagrad update\n",
    " # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savez(\"weights.en\",wxh_en=wxh_en,whh_en=whh_en,why_en=why_en,bh_en=bh_en,by_en=by_en)\n",
    "np.savez(\"weights.fr\",wxh_fr=wxh_fr,whh_fr=whh_fr,why_fr=why_fr,bh_fr=bh_fr,by_fr=by_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
