{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.models as word2vec\n",
    "model_en = word2vec.Word2Vec.load('englishwords')\n",
    "model_fr = word2vec.Word2Vec.load('frenchwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines_en = open('test.en','r',encoding='utf8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_en = open('test.en','r',encoding='utf8').read()\n",
    "data_en = data_en.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english file has 252583 words,300 features\n"
     ]
    }
   ],
   "source": [
    "words_en = list(set(data_en))\n",
    "data_size_en,vocab_size_en = len(data_en),len(model_en.wv['the'])\n",
    "print('english file has %d words,%d features'%(data_size_en,vocab_size_en))\n",
    "\n",
    "word_to_ix_en = {}\n",
    "ix_to_word_en = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec English unique words = 19176, word2vector features = 300\n"
     ]
    }
   ],
   "source": [
    "for w in words_en:\n",
    "    word_to_ix_en[w] = model_en.wv[w]\n",
    "    ix_to_word_en[tuple(model_en.wv[w])] = w\n",
    "print('Word2Vec English unique words = %d, word2vector features = %d'%(len(word_to_ix_en),len(model_en.wv['the'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "french file has 291815 words,300 features\n"
     ]
    }
   ],
   "source": [
    "lines_fr = open('test.fr','r',encoding='utf8').readlines()\n",
    "data_fr = open('test.fr','r',encoding='utf8').read()\n",
    "data_fr = data_fr.split()\n",
    "words_fr = list(set(data_fr))\n",
    "data_size_fr,vocab_size_fr = len(data_fr),len(model_en.wv['la'])\n",
    "print('french file has %d words,%d features'%(data_size_fr,vocab_size_fr))\n",
    "word_to_ix_fr = {}\n",
    "ix_to_word_fr = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec French unique words = 21084\n"
     ]
    }
   ],
   "source": [
    "for w in words_fr:\n",
    "    word_to_ix_fr[w] = model_fr.wv[w]\n",
    "    ix_to_word_fr[tuple(model_fr.wv[w])] = w\n",
    "print('Word2Vec French unique words =',len(word_to_ix_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = len(lines_en)\n",
    "num = len(lines_fr) #No of Sentences\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "def dsigmoid(x):\n",
    "    return x*(1.0-x)\n",
    "def dtanh(x):\n",
    "    return (1.0-x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters which are same for both encoder and decoder\n",
    "hidden_size = 100\n",
    "learning_rate = 1e-1\n",
    "d_en = vocab_size_en\n",
    "d_fr = vocab_size_fr\n",
    "z_en = hidden_size + d_en\n",
    "z_fr = hidden_size + d_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoder weight parameters for english language\n",
    "wf_en = np.random.randn(hidden_size,z_en)*0.01\n",
    "bf_en = np.zeros((hidden_size,1))\n",
    "\n",
    "wi_en = np.random.randn(hidden_size,z_en)*0.01\n",
    "bi_en = np.zeros((hidden_size,1))\n",
    "\n",
    "wc_en = np.random.randn(hidden_size,z_en)*0.01\n",
    "bc_en = np.zeros((hidden_size,1))\n",
    "\n",
    "wo_en = np.random.randn(hidden_size,z_en)*0.01\n",
    "bo_en = np.zeros((hidden_size,1))\n",
    "\n",
    "wy_en = np.random.randn(d_en,hidden_size)*0.01\n",
    "by_en = np.zeros((d_en,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#decoder weight parameters for french language\n",
    "wf_fr = np.random.randn(hidden_size,z_fr)*0.01\n",
    "bf_fr = np.zeros((hidden_size,1))\n",
    "\n",
    "wi_fr = np.random.randn(hidden_size,z_fr)*0.01\n",
    "bi_fr = np.zeros((hidden_size,1))\n",
    "\n",
    "wc_fr = np.random.randn(hidden_size,z_fr)*0.01\n",
    "bc_fr = np.zeros((hidden_size,1))\n",
    "\n",
    "wo_fr = np.random.randn(hidden_size,z_fr)*0.01\n",
    "bo_fr = np.zeros((hidden_size,1))\n",
    "\n",
    "wy_fr = np.random.randn(d_fr,hidden_size)*0.01\n",
    "by_fr = np.zeros((d_fr,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainencoder(inputs, targets, hprev, cprev):\n",
    "  loss_en = 0\n",
    "  xs, hf,hi,ho,hc,h,c, y,temp, ps = {}, {}, {}, {},{},{},{},{},{},{}\n",
    "  h[-1] = np.copy(hprev)\n",
    "  c[-1] = np.copy(cprev)\n",
    "  # forward pass  \n",
    "#   print(len(inputs))\n",
    "  for t in range(5): #len(inputs)\n",
    "    xs[t] = np.zeros((vocab_size_en,1)) # encode in 1-of-k representation\n",
    "    a = np.reshape(inputs[t],(300,1))\n",
    "    xs[t] = np.copy(a)\n",
    "    xs[t] = np.row_stack((h[t-1],xs[t]))\n",
    "    hf[t] = sigmoid(np.dot(wf_en,xs[t])+bf_en)\n",
    "    hi[t] = sigmoid(np.dot(wi_en,xs[t])+bi_en)\n",
    "    ho[t] = sigmoid(np.dot(wo_en,xs[t])+bo_en)\n",
    "    hc[t] = np.tanh(np.dot(wc_en,xs[t])+bc_en)\n",
    "    c[t] = hf[t]*c[t-1] + hi[t]*hc[t]\n",
    "    h[t] = ho[t]*np.tanh(c[t])\n",
    "    y[t] = np.dot(wy_en,h[t])+ by_en ## unnormalized\n",
    "    ps[t] = np.exp(y[t]) / np.sum(np.exp(y[t])) # probabilities for next words\n",
    "    loss_en = loss_en + np.linalg.norm(np.log(ps[t]))\n",
    "#   print(hs)\n",
    "  dWf,dWi,dWo,dWc, dWy = np.zeros_like(wf_en),np.zeros_like(wi_en),np.zeros_like(wo_en),np.zeros_like(wc_en),np.zeros_like(wy_en)\n",
    "  dbf,dbi,dbo,dbc, dby = np.zeros_like(bf_en),np.zeros_like(bi_en),np.zeros_like(bo_en),np.zeros_like(bc_en),np.zeros_like(by_en)\n",
    "  dhnext = np.zeros_like(h[0])\n",
    "  dcnext = np.zeros_like(c[0])\n",
    "  for t in reversed(range(5)): # len(inputs)\n",
    "#     print('e',len(targets))\n",
    "    target = np.reshape(targets[t],(300,1))\n",
    "    dy = np.copy(y[t])\n",
    "    dy -= target  # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWy += np.dot(dy, h[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(wy_en.T,dy)+dhnext\n",
    "    # Gradient for ho\n",
    "    dho = np.tanh(c[t])*dh\n",
    "    dho = dsigmoid(ho[t])*dho\n",
    "    #Gradient for c\n",
    "    dc = ho[t]*dh*dtanh(h[t])\n",
    "    dc = dc + dcnext\n",
    "    #Gradient for hf\n",
    "    dhf = c[t-1]*dc\n",
    "    dhf = dsigmoid(hf[t])*dhf\n",
    "    #Gradient for hi\n",
    "    dhi = hc[t]*dc\n",
    "    dhi = dsigmoid(hi[t])*dhi\n",
    "    #Gradient for hc\n",
    "    dhc = hi[t]*dc\n",
    "    dhc = dtanh(hc[t])*dhc\n",
    "    #Gate gradients\n",
    "    dWf += np.dot(dhf,xs[t].T)\n",
    "    dbf += dhf\n",
    "    dxf = np.dot(wf_en.T,dhf)\n",
    "    \n",
    "    dWi += np.dot(dhi,xs[t].T)\n",
    "    dbi += dhi\n",
    "    dxi = np.dot(wi_en.T,dhi)\n",
    "    \n",
    "    dWo += np.dot(dho,xs[t].T)\n",
    "    dbo += dho\n",
    "    dxo = np.dot(wo_en.T,dho)\n",
    "    \n",
    "    dWc += np.dot(dhc,xs[t].T)\n",
    "    dbc += dhc\n",
    "    dxc = np.dot(wc_en.T,dhc)\n",
    "    \n",
    "    dx = dxo+dxc+dxi+dxf\n",
    "    \n",
    "    dhnext = dx[:hidden_size,:]\n",
    "    dcnext = hf[t]*dc\n",
    "  for dparam in [dWf, dWi, dWo,dWc,dWy,dbf,dbi,dbo,dbc,dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return dWf, dWi, dWo,dWc,dWy,dbf,dbi,dbo,dbc,dby, h[5-1],c[5-1],loss_en #hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traindecoder(inputs, targets, hprev, cprev):\n",
    "  loss_fr = 0\n",
    "  xs,hf,hi,ho,hc,h,c,y,temp,ps = {},{},{},{},{},{},{},{},{},{}\n",
    "  h[-1] = np.copy(hprev)\n",
    "  c[-1] = np.copy(cprev)\n",
    "  # forward pass\n",
    "  for t in range(5): #len(inputs)\n",
    "    xs[t] = np.zeros((vocab_size_fr,1)) # encode in 1-of-k representation\n",
    "    if(int(inputs[0])!=-1):\n",
    "        a = np.reshape(inputs[t],(300,1))\n",
    "        xs[t] = np.copy(a)\n",
    "    xs[t] = np.row_stack((h[t-1],xs[t]))\n",
    "    hf[t] = sigmoid(np.dot(wf_fr,xs[t])+bf_fr)\n",
    "    hi[t] = sigmoid(np.dot(wi_fr,xs[t])+bi_fr)\n",
    "    ho[t] = sigmoid(np.dot(wo_fr,xs[t])+bo_fr)\n",
    "    hc[t] = np.tanh(np.dot(wc_fr,xs[t])+bc_fr)\n",
    "    c[t] = hf[t]*c[t-1] + hi[t]*hc[t]\n",
    "    h[t] = ho[t]*np.tanh(c[t])\n",
    "    y[t] = np.dot(wy_fr,h[t])+ by_fr ## unnormalized\n",
    "    ps[t] = np.exp(y[t]) / np.sum(np.exp(y[t])) # probabilities for next words\n",
    "    loss_fr = loss_fr + np.linalg.norm(np.log(ps[t]))\n",
    "  dWf,dWi,dWo,dWc, dWy = np.zeros_like(wf_en),np.zeros_like(wi_en),np.zeros_like(wo_en),np.zeros_like(wc_en),np.zeros_like(wy_en)\n",
    "  dbf,dbi,dbo,dbc, dby = np.zeros_like(bf_en),np.zeros_like(bi_en),np.zeros_like(bo_en),np.zeros_like(bc_en),np.zeros_like(by_en)\n",
    "  dhnext = np.zeros_like(h[0])\n",
    "  dcnext = np.zeros_like(c[0])\n",
    "  for t in reversed(range(5)):#len(targets)\n",
    "#     print('d',len(targets))\n",
    "    target = np.reshape(targets[t],(300,1))\n",
    "    dy = np.copy(y[t])\n",
    "    dy -= target\n",
    "    dWy += np.dot(dy, h[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(wy_fr.T,dy)+dhnext\n",
    "    # Gradient for ho\n",
    "    dho = np.tanh(c[t])*dh\n",
    "    dho = dsigmoid(ho[t])*dho\n",
    "    #Gradient for c\n",
    "    dc = ho[t]*dh*dtanh(h[t])\n",
    "    dc = dc + dcnext\n",
    "    #Gradient for hf\n",
    "    dhf = c[t-1]*dc\n",
    "    dhf = dsigmoid(hf[t])*dhf\n",
    "    #Gradient for hi\n",
    "    dhi = hc[t]*dc\n",
    "    dhi = dsigmoid(hi[t])*dhi\n",
    "    #Gradient for hc\n",
    "    dhc = hi[t]*dc\n",
    "    dhc = dtanh(hc[t])*dhc\n",
    "    #Gate gradients\n",
    "    dWf += np.dot(dhf,xs[t].T)\n",
    "    dbf += dhf\n",
    "    dxf = np.dot(wf_fr.T,dhf)\n",
    "    \n",
    "    dWi += np.dot(dhi,xs[t].T)\n",
    "    dbi += dhi\n",
    "    dxi = np.dot(wi_fr.T,dhi)\n",
    "    \n",
    "    dWo += np.dot(dho,xs[t].T)\n",
    "    dbo += dho\n",
    "    dxo = np.dot(wo_fr.T,dho)\n",
    "    \n",
    "    dWc += np.dot(dhc,xs[t].T)\n",
    "    dbc += dhc\n",
    "    dxc = np.dot(wc_fr.T,dhc)\n",
    "    \n",
    "    dx = dxo+dxc+dxi+dxf\n",
    "    \n",
    "    dhnext = dx[:hidden_size,:]\n",
    "    dcnext = hf[t]*dc\n",
    "  for dparam in [dWf, dWi, dWo,dWc,dWy,dbf,dbi,dbo,dbc,dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return dWf, dWi, dWo,dWc,dWy,dbf,dbi,dbo,dbc,dby, h[5-1], c[5-1], loss_fr#hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(inputs, targets):\n",
    "  temp,xs,hf,hi,ho,hc,c,h,y = {},{},{},{},{},{},{},{},{}\n",
    "  h[-1] = np.zeros((hidden_size,1))\n",
    "  c[-1] = np.zeros((hidden_size,1))\n",
    "  # forward pass\n",
    "  size = len(inputs)\n",
    "  if(size>5):\n",
    "    size = 5\n",
    "  for t in range(size): #len(inputs)    \n",
    "    a = np.reshape(inputs[t],(300,1))\n",
    "    xs[t] = np.zeros((vocab_size_en,1)) # encode in 1-of-k representation\n",
    "    xs[t] = np.copy(a)\n",
    "    xs[t] = np.row_stack((h[t-1],xs[t]))\n",
    "    hf[t] = sigmoid(np.dot(wf_en,xs[t])+bf_en)\n",
    "    hi[t] = sigmoid(np.dot(wi_en,xs[t])+bi_en)\n",
    "    ho[t] = sigmoid(np.dot(wo_en,xs[t])+bo_en)\n",
    "    hc[t] = np.tanh(np.dot(wc_en,xs[t])+bc_en)\n",
    "    c[t] = hf[t]*c[t-1]+ hc[t]*hi[t]\n",
    "    h[t]  = ho[t]*np.tanh(c[t])\n",
    "    y[t]= np.dot(wy_en, h[t]) + by_en\n",
    "  hprev = h[len(inputs)-1]\n",
    "  cprev = c[len(inputs)-1]\n",
    "  tem = \"\"\n",
    "  ans = \"\"\n",
    "  k = 0\n",
    "  t = -1\n",
    "  while k<5:\n",
    "      temp = np.zeros((vocab_size_fr,1))\n",
    "      if(t!=-1):\n",
    "          temp[t] = 1\n",
    "      xs = np.row_stack((hprev,temp))\n",
    "      hf = sigmoid(np.dot(wf_fr,xs)+bf_fr)\n",
    "      hi = sigmoid(np.dot(wi_fr,xs)+bi_fr)\n",
    "      ho = sigmoid(np.dot(wo_fr,xs)+bo_fr)\n",
    "      hc = np.tanh(np.dot(wc_fr,xs)+bc_fr)\n",
    "      cprev = hf*cprev+ hc*hi\n",
    "      hprev = ho*np.tanh(cprev)\n",
    "      y = np.dot(wy_fr, hprev) + by_fr # unnormalized log probabilities for next chars\n",
    "      p = np.exp(y)/np.sum(np.exp(y))\n",
    "      keys = np.zeros((len(ix_to_word_fr)))\n",
    "      mind = 100000.0\n",
    "      min_key = tuple\n",
    "      for ix in ix_to_word_fr.keys():\n",
    "          cosdist = np.dot(np.asarray(ix).T,y)/(np.linalg.norm(np.asarray(ix))*np.linalg.norm(y))\n",
    "          if(cosdist<mind):\n",
    "                mind = cosdist\n",
    "                minkey = ix\n",
    "#       print(keys.shape,y.shape)\n",
    "#       print(ix_to_word_fr[minkey],mind)\n",
    "      tem = ix_to_word_fr[minkey]\n",
    "#       pr = np.exp(y) / np.sum(np.exp(y))\n",
    "#       print(y.shape,y.reshape(300).shape)\n",
    "#       maxi = y[0][0]\n",
    "#       for i in range(len(words_fr)):\n",
    "#           if y[i][0]>=maxi:\n",
    "#               maxi = y[i][0]\n",
    "#               t = i\n",
    "#       arr = np.asarray(ix_to_word_fr);\n",
    "#       print(arr[np.linalg.norm(arr-y, axis=1).argmin()]) # closest key to y vector is the word\n",
    "      #tem = ix_to_word_fr[tuple(y.reshape(300))] # J Put word in it. find the closest word to y(vector)\n",
    "      k = k + 1\n",
    "      ans = ans + \" \" +tem\n",
    "  return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n,p = 0,0\n",
    "mdWf_en,mdWi_en,mdWo_en,mdWc_en, mdWy_en = np.zeros_like(wf_en),np.zeros_like(wi_en),np.zeros_like(wo_en),np.zeros_like(wc_en),np.zeros_like(wy_en)\n",
    "mdbf_en,mdbi_en,mdbo_en,mdbc_en, mdby_en = np.zeros_like(bf_en),np.zeros_like(bi_en),np.zeros_like(bo_en),np.zeros_like(bc_en),np.zeros_like(by_en) \n",
    "mdWf_fr,mdWi_fr,mdWo_fr,mdWc_fr, mdWy_fr = np.zeros_like(wf_fr),np.zeros_like(wi_fr),np.zeros_like(wo_fr),np.zeros_like(wc_fr),np.zeros_like(wy_fr)\n",
    "mdbf_fr,mdbi_fr,mdbo_fr,mdbc_fr, mdby_fr = np.zeros_like(bf_fr),np.zeros_like(bi_fr),np.zeros_like(bo_fr),np.zeros_like(bc_fr),np.zeros_like(by_fr)  \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Training...iteration:0\n",
      "loss_en=494.085572\n",
      "loss_fr=494.089102\n",
      " philanthropiques. philanthropiques. philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:1\n",
      "loss_en=494.087334\n",
      "loss_fr=494.090928\n",
      " philanthropiques. philanthropiques. philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:2\n",
      "loss_en=494.089070\n",
      "loss_fr=494.091401\n",
      " détection philanthropiques. philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:3\n",
      "loss_en=494.090831\n",
      "loss_fr=494.091154\n",
      " détection philanthropiques. philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:4\n",
      "loss_en=494.092491\n",
      "loss_fr=494.090784\n",
      " fourniront fourniront philanthropiques. fourniront fourniront\n",
      ">>Training...iteration:5\n",
      "loss_en=494.093973\n",
      "loss_fr=494.090585\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:6\n",
      "loss_en=494.095174\n",
      "loss_fr=494.090723\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:7\n",
      "loss_en=494.096140\n",
      "loss_fr=494.091135\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:8\n",
      "loss_en=494.096941\n",
      "loss_fr=494.091579\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:9\n",
      "loss_en=494.097616\n",
      "loss_fr=494.091884\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:10\n",
      "loss_en=494.098187\n",
      "loss_fr=494.092045\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:11\n",
      "loss_en=494.098672\n",
      "loss_fr=494.092113\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:12\n",
      "loss_en=494.099089\n",
      "loss_fr=494.092135\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:13\n",
      "loss_en=494.099453\n",
      "loss_fr=494.092143\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:14\n",
      "loss_en=494.099775\n",
      "loss_fr=494.092158\n",
      " fourniront fourniront fourniront fourniront secteur.\n",
      ">>Training...iteration:15\n",
      "loss_en=494.100062\n",
      "loss_fr=494.092182\n",
      " fourniront fourniront fourniront fourniront secteur.\n",
      ">>Training...iteration:16\n",
      "loss_en=494.100314\n",
      "loss_fr=494.092212\n",
      " fourniront fourniront fourniront fourniront secteur.\n",
      ">>Training...iteration:17\n",
      "loss_en=494.100533\n",
      "loss_fr=494.092239\n",
      " fourniront fourniront fourniront fourniront secteur.\n",
      ">>Training...iteration:18\n",
      "loss_en=494.100722\n",
      "loss_fr=494.092259\n",
      " fourniront fourniront fourniront fourniront secteur.\n",
      ">>Training...iteration:19\n",
      "loss_en=494.100882\n",
      "loss_fr=494.092269\n",
      " fourniront fourniront fourniront fourniront secteur.\n",
      ">>Training...iteration:20\n",
      "loss_en=494.101020\n",
      "loss_fr=494.092273\n",
      " fourniront fourniront fourniront fourniront secteur.\n",
      ">>Training...iteration:21\n",
      "loss_en=494.101137\n",
      "loss_fr=494.092275\n",
      " fourniront fourniront fourniront secteur. secteur.\n",
      ">>Training...iteration:22\n",
      "loss_en=494.101240\n",
      "loss_fr=494.092278\n",
      " fourniront fourniront fourniront secteur. secteur.\n",
      ">>Training...iteration:23\n",
      "loss_en=494.101331\n",
      "loss_fr=494.092287\n",
      " fourniront fourniront fourniront secteur. secteur.\n",
      ">>Training...iteration:24\n",
      "loss_en=494.101413\n",
      "loss_fr=494.092303\n",
      " fourniront fourniront fourniront secteur. secteur.\n",
      ">>Training...iteration:25\n",
      "loss_en=494.101488\n",
      "loss_fr=494.092329\n",
      " fourniront fourniront fourniront secteur. secteur.\n",
      ">>Training...iteration:26\n",
      "loss_en=494.101558\n",
      "loss_fr=494.092364\n",
      " fourniront fourniront fourniront secteur. secteur.\n",
      ">>Training...iteration:27\n",
      "loss_en=494.101624\n",
      "loss_fr=494.092410\n",
      " fourniront fourniront fourniront secteur. secteur.\n",
      ">>Training...iteration:28\n",
      "loss_en=494.101687\n",
      "loss_fr=494.092465\n",
      " fourniront fourniront fourniront secteur. secteur.\n",
      ">>Training...iteration:29\n",
      "loss_en=494.101746\n",
      "loss_fr=494.092527\n",
      " fourniront fourniront fourniront secteur. secteur.\n",
      ">>Training...iteration:30\n",
      "loss_en=494.101804\n",
      "loss_fr=494.092594\n",
      " détection fourniront fourniront secteur. secteur.\n",
      ">>Training...iteration:31\n",
      "loss_en=494.101861\n",
      "loss_fr=494.092665\n",
      " détection philanthropiques. fourniront secteur. secteur.\n",
      ">>Training...iteration:32\n",
      "loss_en=494.101917\n",
      "loss_fr=494.092737\n",
      " détection philanthropiques. fourniront secteur. secteur.\n",
      ">>Training...iteration:33\n",
      "loss_en=494.101973\n",
      "loss_fr=494.092808\n",
      " détection philanthropiques. fourniront secteur. secteur.\n",
      ">>Training...iteration:34\n",
      "loss_en=494.102029\n",
      "loss_fr=494.092878\n",
      " détection philanthropiques. fourniront secteur. secteur.\n",
      ">>Training...iteration:35\n",
      "loss_en=494.102084\n",
      "loss_fr=494.092945\n",
      " détection philanthropiques. philanthropiques. secteur. secteur.\n",
      ">>Training...iteration:36\n",
      "loss_en=494.102140\n",
      "loss_fr=494.093008\n",
      " détection philanthropiques. philanthropiques. secteur. secteur.\n",
      ">>Training...iteration:37\n",
      "loss_en=494.102195\n",
      "loss_fr=494.093069\n",
      " détection philanthropiques. philanthropiques. secteur. secteur.\n",
      ">>Training...iteration:38\n",
      "loss_en=494.102249\n",
      "loss_fr=494.093128\n",
      " détection philanthropiques. philanthropiques. secteur. secteur.\n",
      ">>Training...iteration:39\n",
      "loss_en=494.102304\n",
      "loss_fr=494.093188\n",
      " détection philanthropiques. philanthropiques. secteur. secteur.\n",
      ">>Training...iteration:40\n",
      "loss_en=494.102357\n",
      "loss_fr=494.093251\n",
      " détection philanthropiques. philanthropiques. secteur. secteur.\n",
      ">>Training...iteration:41\n",
      "loss_en=494.102411\n",
      "loss_fr=494.093317\n",
      " détection philanthropiques. philanthropiques. secteur. secteur.\n",
      ">>Training...iteration:42\n",
      "loss_en=494.102464\n",
      "loss_fr=494.093389\n",
      " détection philanthropiques. secteur. secteur. secteur.\n",
      ">>Training...iteration:43\n",
      "loss_en=494.102517\n",
      "loss_fr=494.093466\n",
      " détection philanthropiques. secteur. secteur. secteur.\n",
      ">>Training...iteration:44\n",
      "loss_en=494.102570\n",
      "loss_fr=494.093550\n",
      " détection philanthropiques. secteur. secteur. secteur.\n",
      ">>Training...iteration:45\n",
      "loss_en=494.102623\n",
      "loss_fr=494.093640\n",
      " détection philanthropiques. secteur. secteur. secteur.\n",
      ">>Training...iteration:46\n",
      "loss_en=494.102676\n",
      "loss_fr=494.093737\n",
      " détection philanthropiques. secteur. secteur. secteur.\n",
      ">>Training...iteration:47\n",
      "loss_en=494.102729\n",
      "loss_fr=494.093841\n",
      " détection philanthropiques. secteur. secteur. secteur.\n",
      ">>Training...iteration:48\n",
      "loss_en=494.102782\n",
      "loss_fr=494.093952\n",
      " détection philanthropiques. secteur. secteur. secteur.\n",
      ">>Training...iteration:49\n",
      "loss_en=494.102836\n",
      "loss_fr=494.094071\n",
      " détection philanthropiques. secteur. secteur. secteur.\n",
      ">>Training...iteration:50\n",
      "loss_en=494.102890\n",
      "loss_fr=494.094198\n",
      " détection philanthropiques. secteur. secteur. secteur.\n",
      ">>Training...iteration:51\n",
      "loss_en=494.102945\n",
      "loss_fr=494.094330\n",
      " détection philanthropiques. secteur. secteur. secteur.\n",
      ">>Training...iteration:52\n",
      "loss_en=494.102999\n",
      "loss_fr=494.094468\n",
      " détection philanthropiques. secteur. secteur. 787\n",
      ">>Training...iteration:53\n",
      "loss_en=494.103055\n",
      "loss_fr=494.094608\n",
      " détection philanthropiques. secteur. secteur. 787\n",
      ">>Training...iteration:54\n",
      "loss_en=494.103110\n",
      "loss_fr=494.094750\n",
      " détection philanthropiques. secteur. secteur. 787\n",
      ">>Training...iteration:55\n",
      "loss_en=494.103166\n",
      "loss_fr=494.094891\n",
      " détection philanthropiques. secteur. secteur. 787\n",
      ">>Training...iteration:56\n",
      "loss_en=494.103222\n",
      "loss_fr=494.095030\n",
      " détection philanthropiques. secteur. secteur. 787\n",
      ">>Training...iteration:57\n",
      "loss_en=494.103278\n",
      "loss_fr=494.095166\n",
      " détection philanthropiques. secteur. secteur. 787\n",
      ">>Training...iteration:58\n",
      "loss_en=494.103335\n",
      "loss_fr=494.095298\n",
      " détection philanthropiques. secteur. secteur. 787\n",
      ">>Training...iteration:59\n",
      "loss_en=494.103391\n",
      "loss_fr=494.095425\n",
      " détection philanthropiques. secteur. fourniront 787\n",
      ">>Training...iteration:60\n",
      "loss_en=494.103447\n",
      "loss_fr=494.095548\n",
      " détection philanthropiques. secteur. fourniront 787\n",
      ">>Training...iteration:61\n",
      "loss_en=494.103502\n",
      "loss_fr=494.095666\n",
      " détection philanthropiques. secteur. fourniront 787\n",
      ">>Training...iteration:62\n",
      "loss_en=494.103557\n",
      "loss_fr=494.095779\n",
      " détection philanthropiques. secteur. fourniront secteur.\n",
      ">>Training...iteration:63\n",
      "loss_en=494.103611\n",
      "loss_fr=494.095888\n",
      " détection philanthropiques. secteur. fourniront fourniront\n",
      ">>Training...iteration:64\n",
      "loss_en=494.103665\n",
      "loss_fr=494.095993\n",
      " détection philanthropiques. secteur. fourniront fourniront\n",
      ">>Training...iteration:65\n",
      "loss_en=494.103717\n",
      "loss_fr=494.096094\n",
      " détection philanthropiques. secteur. fourniront fourniront\n",
      ">>Training...iteration:66\n",
      "loss_en=494.103769\n",
      "loss_fr=494.096192\n",
      " détection philanthropiques. fourniront fourniront fourniront\n",
      ">>Training...iteration:67\n",
      "loss_en=494.103819\n",
      "loss_fr=494.096286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " détection philanthropiques. fourniront fourniront fourniront\n",
      ">>Training...iteration:68\n",
      "loss_en=494.103868\n",
      "loss_fr=494.096377\n",
      " détection philanthropiques. fourniront fourniront fourniront\n",
      ">>Training...iteration:69\n",
      "loss_en=494.103915\n",
      "loss_fr=494.096466\n",
      " détection philanthropiques. fourniront fourniront fourniront\n",
      ">>Training...iteration:70\n",
      "loss_en=494.103961\n",
      "loss_fr=494.096551\n",
      " détection philanthropiques. fourniront fourniront fourniront\n",
      ">>Training...iteration:71\n",
      "loss_en=494.104004\n",
      "loss_fr=494.096635\n",
      " détection philanthropiques. fourniront fourniront fourniront\n",
      ">>Training...iteration:72\n",
      "loss_en=494.104047\n",
      "loss_fr=494.096716\n",
      " détection philanthropiques. fourniront fourniront fourniront\n",
      ">>Training...iteration:73\n",
      "loss_en=494.104087\n",
      "loss_fr=494.096795\n",
      " détection philanthropiques. fourniront fourniront fourniront\n",
      ">>Training...iteration:74\n",
      "loss_en=494.104125\n",
      "loss_fr=494.096872\n",
      " détection philanthropiques. fourniront fourniront fourniront\n",
      ">>Training...iteration:75\n",
      "loss_en=494.104161\n",
      "loss_fr=494.096947\n",
      " détection philanthropiques. fourniront fourniront fourniront\n",
      ">>Training...iteration:76\n",
      "loss_en=494.104195\n",
      "loss_fr=494.097021\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:77\n",
      "loss_en=494.104227\n",
      "loss_fr=494.097094\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:78\n",
      "loss_en=494.104257\n",
      "loss_fr=494.097165\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:79\n",
      "loss_en=494.104285\n",
      "loss_fr=494.097235\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:80\n",
      "loss_en=494.104311\n",
      "loss_fr=494.097303\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:81\n",
      "loss_en=494.104335\n",
      "loss_fr=494.097370\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:82\n",
      "loss_en=494.104358\n",
      "loss_fr=494.097436\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:83\n",
      "loss_en=494.104378\n",
      "loss_fr=494.097500\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:84\n",
      "loss_en=494.104397\n",
      "loss_fr=494.097563\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:85\n",
      "loss_en=494.104414\n",
      "loss_fr=494.097623\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:86\n",
      "loss_en=494.104430\n",
      "loss_fr=494.097682\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:87\n",
      "loss_en=494.104445\n",
      "loss_fr=494.097739\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:88\n",
      "loss_en=494.104458\n",
      "loss_fr=494.097794\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:89\n",
      "loss_en=494.104470\n",
      "loss_fr=494.097847\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:90\n",
      "loss_en=494.104482\n",
      "loss_fr=494.097898\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:91\n",
      "loss_en=494.104492\n",
      "loss_fr=494.097947\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:92\n",
      "loss_en=494.104502\n",
      "loss_fr=494.097995\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:93\n",
      "loss_en=494.104511\n",
      "loss_fr=494.098040\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:94\n",
      "loss_en=494.104519\n",
      "loss_fr=494.098083\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:95\n",
      "loss_en=494.104527\n",
      "loss_fr=494.098125\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:96\n",
      "loss_en=494.104534\n",
      "loss_fr=494.098165\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:97\n",
      "loss_en=494.104541\n",
      "loss_fr=494.098202\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:98\n",
      "loss_en=494.104548\n",
      "loss_fr=494.098238\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:99\n",
      "loss_en=494.104553\n",
      "loss_fr=494.098272\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:100\n",
      "loss_en=494.104559\n",
      "loss_fr=494.098303\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:101\n",
      "loss_en=494.104564\n",
      "loss_fr=494.098333\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:102\n",
      "loss_en=494.104569\n",
      "loss_fr=494.098359\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:103\n",
      "loss_en=494.104574\n",
      "loss_fr=494.098383\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:104\n",
      "loss_en=494.104578\n",
      "loss_fr=494.098405\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:105\n",
      "loss_en=494.104582\n",
      "loss_fr=494.098423\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:106\n",
      "loss_en=494.104585\n",
      "loss_fr=494.098439\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:107\n",
      "loss_en=494.104589\n",
      "loss_fr=494.098452\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:108\n",
      "loss_en=494.104592\n",
      "loss_fr=494.098463\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:109\n",
      "loss_en=494.104595\n",
      "loss_fr=494.098471\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:110\n",
      "loss_en=494.104597\n",
      "loss_fr=494.098476\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:111\n",
      "loss_en=494.104600\n",
      "loss_fr=494.098479\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:112\n",
      "loss_en=494.104602\n",
      "loss_fr=494.098480\n",
      " fourniront fourniront fourniront fourniront philanthropiques.\n",
      ">>Training...iteration:113\n",
      "loss_en=494.104603\n",
      "loss_fr=494.098480\n",
      " fourniront fourniront fourniront fourniront philanthropiques.\n",
      ">>Training...iteration:114\n",
      "loss_en=494.104605\n",
      "loss_fr=494.098478\n",
      " fourniront fourniront fourniront fourniront philanthropiques.\n",
      ">>Training...iteration:115\n",
      "loss_en=494.104606\n",
      "loss_fr=494.098476\n",
      " fourniront fourniront fourniront fourniront philanthropiques.\n",
      ">>Training...iteration:116\n",
      "loss_en=494.104607\n",
      "loss_fr=494.098474\n",
      " fourniront fourniront fourniront fourniront philanthropiques.\n",
      ">>Training...iteration:117\n",
      "loss_en=494.104607\n",
      "loss_fr=494.098471\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:118\n",
      "loss_en=494.104608\n",
      "loss_fr=494.098469\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:119\n",
      "loss_en=494.104607\n",
      "loss_fr=494.098468\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:120\n",
      "loss_en=494.104606\n",
      "loss_fr=494.098469\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:121\n",
      "loss_en=494.104605\n",
      "loss_fr=494.098471\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:122\n",
      "loss_en=494.104603\n",
      "loss_fr=494.098476\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:123\n",
      "loss_en=494.104601\n",
      "loss_fr=494.098483\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:124\n",
      "loss_en=494.104598\n",
      "loss_fr=494.098493\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:125\n",
      "loss_en=494.104595\n",
      "loss_fr=494.098506\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:126\n",
      "loss_en=494.104591\n",
      "loss_fr=494.098521\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:127\n",
      "loss_en=494.104586\n",
      "loss_fr=494.098540\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:128\n",
      "loss_en=494.104581\n",
      "loss_fr=494.098562\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:129\n",
      "loss_en=494.104575\n",
      "loss_fr=494.098587\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:130\n",
      "loss_en=494.104569\n",
      "loss_fr=494.098615\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:131\n",
      "loss_en=494.104562\n",
      "loss_fr=494.098646\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:132\n",
      "loss_en=494.104555\n",
      "loss_fr=494.098680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:133\n",
      "loss_en=494.104547\n",
      "loss_fr=494.098717\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:134\n",
      "loss_en=494.104539\n",
      "loss_fr=494.098757\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:135\n",
      "loss_en=494.104531\n",
      "loss_fr=494.098799\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:136\n",
      "loss_en=494.104523\n",
      "loss_fr=494.098844\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:137\n",
      "loss_en=494.104514\n",
      "loss_fr=494.098890\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:138\n",
      "loss_en=494.104505\n",
      "loss_fr=494.098938\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:139\n",
      "loss_en=494.104497\n",
      "loss_fr=494.098987\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:140\n",
      "loss_en=494.104488\n",
      "loss_fr=494.099037\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:141\n",
      "loss_en=494.104480\n",
      "loss_fr=494.099087\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:142\n",
      "loss_en=494.104471\n",
      "loss_fr=494.099137\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:143\n",
      "loss_en=494.104463\n",
      "loss_fr=494.099187\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:144\n",
      "loss_en=494.104456\n",
      "loss_fr=494.099236\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:145\n",
      "loss_en=494.104448\n",
      "loss_fr=494.099284\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:146\n",
      "loss_en=494.104442\n",
      "loss_fr=494.099330\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:147\n",
      "loss_en=494.104435\n",
      "loss_fr=494.099376\n",
      " l'ignorance fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:148\n",
      "loss_en=494.104430\n",
      "loss_fr=494.099420\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:149\n",
      "loss_en=494.104425\n",
      "loss_fr=494.099462\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:150\n",
      "loss_en=494.104420\n",
      "loss_fr=494.099502\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:151\n",
      "loss_en=494.104417\n",
      "loss_fr=494.099541\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:152\n",
      "loss_en=494.104414\n",
      "loss_fr=494.099579\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:153\n",
      "loss_en=494.104412\n",
      "loss_fr=494.099615\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:154\n",
      "loss_en=494.104411\n",
      "loss_fr=494.099649\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:155\n",
      "loss_en=494.104410\n",
      "loss_fr=494.099683\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:156\n",
      "loss_en=494.104411\n",
      "loss_fr=494.099715\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:157\n",
      "loss_en=494.104412\n",
      "loss_fr=494.099747\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:158\n",
      "loss_en=494.104414\n",
      "loss_fr=494.099778\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:159\n",
      "loss_en=494.104417\n",
      "loss_fr=494.099809\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:160\n",
      "loss_en=494.104421\n",
      "loss_fr=494.099839\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:161\n",
      "loss_en=494.104425\n",
      "loss_fr=494.099869\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:162\n",
      "loss_en=494.104431\n",
      "loss_fr=494.099900\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:163\n",
      "loss_en=494.104437\n",
      "loss_fr=494.099930\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:164\n",
      "loss_en=494.104444\n",
      "loss_fr=494.099961\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:165\n",
      "loss_en=494.104452\n",
      "loss_fr=494.099993\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:166\n",
      "loss_en=494.104461\n",
      "loss_fr=494.100026\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:167\n",
      "loss_en=494.104470\n",
      "loss_fr=494.100059\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:168\n",
      "loss_en=494.104480\n",
      "loss_fr=494.100094\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:169\n",
      "loss_en=494.104491\n",
      "loss_fr=494.100129\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:170\n",
      "loss_en=494.104502\n",
      "loss_fr=494.100166\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:171\n",
      "loss_en=494.104515\n",
      "loss_fr=494.100203\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:172\n",
      "loss_en=494.104528\n",
      "loss_fr=494.100242\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:173\n",
      "loss_en=494.104541\n",
      "loss_fr=494.100282\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:174\n",
      "loss_en=494.104555\n",
      "loss_fr=494.100323\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:175\n",
      "loss_en=494.104570\n",
      "loss_fr=494.100366\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:176\n",
      "loss_en=494.104585\n",
      "loss_fr=494.100409\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:177\n",
      "loss_en=494.104601\n",
      "loss_fr=494.100454\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:178\n",
      "loss_en=494.104618\n",
      "loss_fr=494.100499\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:179\n",
      "loss_en=494.104635\n",
      "loss_fr=494.100545\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:180\n",
      "loss_en=494.104653\n",
      "loss_fr=494.100592\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:181\n",
      "loss_en=494.104671\n",
      "loss_fr=494.100639\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:182\n",
      "loss_en=494.104690\n",
      "loss_fr=494.100687\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:183\n",
      "loss_en=494.104709\n",
      "loss_fr=494.100736\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:184\n",
      "loss_en=494.104729\n",
      "loss_fr=494.100784\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:185\n",
      "loss_en=494.104749\n",
      "loss_fr=494.100833\n",
      " l'ignorance fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:186\n",
      "loss_en=494.104770\n",
      "loss_fr=494.100882\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:187\n",
      "loss_en=494.104791\n",
      "loss_fr=494.100930\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:188\n",
      "loss_en=494.104813\n",
      "loss_fr=494.100979\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:189\n",
      "loss_en=494.104835\n",
      "loss_fr=494.101027\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:190\n",
      "loss_en=494.104858\n",
      "loss_fr=494.101075\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:191\n",
      "loss_en=494.104881\n",
      "loss_fr=494.101123\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Training...iteration:192\n",
      "loss_en=494.104905\n",
      "loss_fr=494.101170\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:193\n",
      "loss_en=494.104929\n",
      "loss_fr=494.101217\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:194\n",
      "loss_en=494.104953\n",
      "loss_fr=494.101263\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:195\n",
      "loss_en=494.104978\n",
      "loss_fr=494.101309\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:196\n",
      "loss_en=494.105003\n",
      "loss_fr=494.101354\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:197\n",
      "loss_en=494.105028\n",
      "loss_fr=494.101398\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:198\n",
      "loss_en=494.105054\n",
      "loss_fr=494.101442\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:199\n",
      "loss_en=494.105080\n",
      "loss_fr=494.101485\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:200\n",
      "loss_en=494.105107\n",
      "loss_fr=494.101528\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:201\n",
      "loss_en=494.105133\n",
      "loss_fr=494.101570\n",
      " fourniront fourniront fourniront philanthropiques. philanthropiques.\n",
      ">>Training...iteration:202\n",
      "loss_en=494.105160\n",
      "loss_fr=494.101611\n",
      " fourniront fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:203\n",
      "loss_en=494.105187\n",
      "loss_fr=494.101651\n",
      " fourniront fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:204\n",
      "loss_en=494.105215\n",
      "loss_fr=494.101691\n",
      " fourniront fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:205\n",
      "loss_en=494.105242\n",
      "loss_fr=494.101730\n",
      " fourniront fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:206\n",
      "loss_en=494.105270\n",
      "loss_fr=494.101768\n",
      " fourniront fourniront philanthropiques. philanthropiques. philanthropiques.\n",
      ">>Training...iteration:207\n",
      "loss_en=494.105298\n",
      "loss_fr=494.101806\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:208\n",
      "loss_en=494.105326\n",
      "loss_fr=494.101842\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:209\n",
      "loss_en=494.105354\n",
      "loss_fr=494.101878\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:210\n",
      "loss_en=494.105382\n",
      "loss_fr=494.101913\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:211\n",
      "loss_en=494.105410\n",
      "loss_fr=494.101947\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:212\n",
      "loss_en=494.105438\n",
      "loss_fr=494.101980\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:213\n",
      "loss_en=494.105466\n",
      "loss_fr=494.102012\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:214\n",
      "loss_en=494.105494\n",
      "loss_fr=494.102043\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:215\n",
      "loss_en=494.105523\n",
      "loss_fr=494.102073\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:216\n",
      "loss_en=494.105551\n",
      "loss_fr=494.102102\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:217\n",
      "loss_en=494.105578\n",
      "loss_fr=494.102130\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:218\n",
      "loss_en=494.105606\n",
      "loss_fr=494.102157\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:219\n",
      "loss_en=494.105634\n",
      "loss_fr=494.102183\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:220\n",
      "loss_en=494.105661\n",
      "loss_fr=494.102208\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:221\n",
      "loss_en=494.105689\n",
      "loss_fr=494.102232\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:222\n",
      "loss_en=494.105716\n",
      "loss_fr=494.102255\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:223\n",
      "loss_en=494.105743\n",
      "loss_fr=494.102276\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:224\n",
      "loss_en=494.105769\n",
      "loss_fr=494.102297\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:225\n",
      "loss_en=494.105796\n",
      "loss_fr=494.102317\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:226\n",
      "loss_en=494.105822\n",
      "loss_fr=494.102335\n",
      " fourniront fourniront philanthropiques. philanthropiques. fourniront\n",
      ">>Training...iteration:227\n",
      "loss_en=494.105848\n",
      "loss_fr=494.102353\n",
      " fourniront fourniront philanthropiques. secteur. fourniront\n",
      ">>Training...iteration:228\n",
      "loss_en=494.105874\n",
      "loss_fr=494.102370\n",
      " fourniront fourniront philanthropiques. secteur. fourniront\n",
      ">>Training...iteration:229\n",
      "loss_en=494.105899\n",
      "loss_fr=494.102385\n",
      " fourniront fourniront philanthropiques. secteur. fourniront\n",
      ">>Training...iteration:230\n",
      "loss_en=494.105924\n",
      "loss_fr=494.102400\n",
      " fourniront fourniront philanthropiques. secteur. fourniront\n",
      ">>Training...iteration:231\n",
      "loss_en=494.105949\n",
      "loss_fr=494.102415\n",
      " fourniront fourniront philanthropiques. secteur. fourniront\n",
      ">>Training...iteration:232\n",
      "loss_en=494.105973\n",
      "loss_fr=494.102428\n",
      " fourniront fourniront philanthropiques. secteur. fourniront\n",
      ">>Training...iteration:233\n",
      "loss_en=494.105997\n",
      "loss_fr=494.102440\n",
      " fourniront fourniront philanthropiques. secteur. fourniront\n",
      ">>Training...iteration:234\n",
      "loss_en=494.106021\n",
      "loss_fr=494.102452\n",
      " fourniront fourniront philanthropiques. fourniront fourniront\n",
      ">>Training...iteration:235\n",
      "loss_en=494.106045\n",
      "loss_fr=494.102463\n",
      " fourniront fourniront philanthropiques. fourniront fourniront\n",
      ">>Training...iteration:236\n",
      "loss_en=494.106068\n",
      "loss_fr=494.102474\n",
      " fourniront fourniront philanthropiques. fourniront fourniront\n",
      ">>Training...iteration:237\n",
      "loss_en=494.106091\n",
      "loss_fr=494.102483\n",
      " fourniront fourniront philanthropiques. fourniront fourniront\n",
      ">>Training...iteration:238\n",
      "loss_en=494.106113\n",
      "loss_fr=494.102492\n",
      " fourniront fourniront philanthropiques. fourniront fourniront\n",
      ">>Training...iteration:239\n",
      "loss_en=494.106135\n",
      "loss_fr=494.102501\n",
      " fourniront fourniront philanthropiques. fourniront fourniront\n",
      ">>Training...iteration:240\n",
      "loss_en=494.106157\n",
      "loss_fr=494.102509\n",
      " fourniront fourniront philanthropiques. fourniront fourniront\n",
      ">>Training...iteration:241\n",
      "loss_en=494.106179\n",
      "loss_fr=494.102517\n",
      " fourniront fourniront philanthropiques. fourniront fourniront\n",
      ">>Training...iteration:242\n",
      "loss_en=494.106200\n",
      "loss_fr=494.102524\n",
      " fourniront fourniront philanthropiques. fourniront fourniront\n",
      ">>Training...iteration:243\n",
      "loss_en=494.106221\n",
      "loss_fr=494.102530\n",
      " fourniront fourniront philanthropiques. fourniront fourniront\n",
      ">>Training...iteration:244\n",
      "loss_en=494.106241\n",
      "loss_fr=494.102537\n",
      " fourniront fourniront philanthropiques. fourniront fourniront\n",
      ">>Training...iteration:245\n",
      "loss_en=494.106262\n",
      "loss_fr=494.102542\n",
      " fourniront fourniront philanthropiques. fourniront fourniront\n",
      ">>Training...iteration:246\n",
      "loss_en=494.106281\n",
      "loss_fr=494.102548\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:247\n",
      "loss_en=494.106301\n",
      "loss_fr=494.102553\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:248\n",
      "loss_en=494.106320\n",
      "loss_fr=494.102558\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:249\n",
      "loss_en=494.106339\n",
      "loss_fr=494.102562\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:250\n",
      "loss_en=494.106358\n",
      "loss_fr=494.102566\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:251\n",
      "loss_en=494.106376\n",
      "loss_fr=494.102570\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:252\n",
      "loss_en=494.106395\n",
      "loss_fr=494.102574\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:253\n",
      "loss_en=494.106412\n",
      "loss_fr=494.102577\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:254\n",
      "loss_en=494.106430\n",
      "loss_fr=494.102580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:255\n",
      "loss_en=494.106447\n",
      "loss_fr=494.102583\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:256\n",
      "loss_en=494.106464\n",
      "loss_fr=494.102585\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:257\n",
      "loss_en=494.106480\n",
      "loss_fr=494.102587\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:258\n",
      "loss_en=494.106497\n",
      "loss_fr=494.102589\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:259\n",
      "loss_en=494.106513\n",
      "loss_fr=494.102591\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:260\n",
      "loss_en=494.106528\n",
      "loss_fr=494.102592\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:261\n",
      "loss_en=494.106544\n",
      "loss_fr=494.102594\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:262\n",
      "loss_en=494.106559\n",
      "loss_fr=494.102595\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:263\n",
      "loss_en=494.106574\n",
      "loss_fr=494.102595\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:264\n",
      "loss_en=494.106588\n",
      "loss_fr=494.102596\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:265\n",
      "loss_en=494.106602\n",
      "loss_fr=494.102596\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:266\n",
      "loss_en=494.106616\n",
      "loss_fr=494.102596\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:267\n",
      "loss_en=494.106630\n",
      "loss_fr=494.102596\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:268\n",
      "loss_en=494.106643\n",
      "loss_fr=494.102595\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:269\n",
      "loss_en=494.106656\n",
      "loss_fr=494.102594\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:270\n",
      "loss_en=494.106668\n",
      "loss_fr=494.102593\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:271\n",
      "loss_en=494.106680\n",
      "loss_fr=494.102592\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:272\n",
      "loss_en=494.106692\n",
      "loss_fr=494.102590\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:273\n",
      "loss_en=494.106704\n",
      "loss_fr=494.102589\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:274\n",
      "loss_en=494.106715\n",
      "loss_fr=494.102587\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:275\n",
      "loss_en=494.106726\n",
      "loss_fr=494.102584\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:276\n",
      "loss_en=494.106737\n",
      "loss_fr=494.102582\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:277\n",
      "loss_en=494.106747\n",
      "loss_fr=494.102579\n",
      " fourniront fourniront secteur. fourniront fourniront\n",
      ">>Training...iteration:278\n",
      "loss_en=494.106757\n",
      "loss_fr=494.102576\n",
      " fourniront fourniront secteur. fourniront fourniront\n",
      ">>Training...iteration:279\n",
      "loss_en=494.106767\n",
      "loss_fr=494.102573\n",
      " fourniront fourniront secteur. fourniront fourniront\n",
      ">>Training...iteration:280\n",
      "loss_en=494.106776\n",
      "loss_fr=494.102570\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:281\n",
      "loss_en=494.106785\n",
      "loss_fr=494.102567\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:282\n",
      "loss_en=494.106794\n",
      "loss_fr=494.102563\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:283\n",
      "loss_en=494.106802\n",
      "loss_fr=494.102560\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:284\n",
      "loss_en=494.106810\n",
      "loss_fr=494.102556\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:285\n",
      "loss_en=494.106818\n",
      "loss_fr=494.102552\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:286\n",
      "loss_en=494.106825\n",
      "loss_fr=494.102549\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:287\n",
      "loss_en=494.106832\n",
      "loss_fr=494.102545\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:288\n",
      "loss_en=494.106839\n",
      "loss_fr=494.102541\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:289\n",
      "loss_en=494.106845\n",
      "loss_fr=494.102537\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:290\n",
      "loss_en=494.106851\n",
      "loss_fr=494.102533\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:291\n",
      "loss_en=494.106857\n",
      "loss_fr=494.102529\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:292\n",
      "loss_en=494.106862\n",
      "loss_fr=494.102526\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:293\n",
      "loss_en=494.106867\n",
      "loss_fr=494.102522\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:294\n",
      "loss_en=494.106872\n",
      "loss_fr=494.102518\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:295\n",
      "loss_en=494.106877\n",
      "loss_fr=494.102515\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:296\n",
      "loss_en=494.106881\n",
      "loss_fr=494.102511\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:297\n",
      "loss_en=494.106885\n",
      "loss_fr=494.102508\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:298\n",
      "loss_en=494.106889\n",
      "loss_fr=494.102505\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:299\n",
      "loss_en=494.106892\n",
      "loss_fr=494.102501\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:300\n",
      "loss_en=494.106895\n",
      "loss_fr=494.102498\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:301\n",
      "loss_en=494.106898\n",
      "loss_fr=494.102496\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:302\n",
      "loss_en=494.106901\n",
      "loss_fr=494.102493\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:303\n",
      "loss_en=494.106903\n",
      "loss_fr=494.102491\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:304\n",
      "loss_en=494.106905\n",
      "loss_fr=494.102488\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:305\n",
      "loss_en=494.106907\n",
      "loss_fr=494.102486\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:306\n",
      "loss_en=494.106909\n",
      "loss_fr=494.102485\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:307\n",
      "loss_en=494.106910\n",
      "loss_fr=494.102483\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:308\n",
      "loss_en=494.106911\n",
      "loss_fr=494.102482\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:309\n",
      "loss_en=494.106912\n",
      "loss_fr=494.102480\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:310\n",
      "loss_en=494.106913\n",
      "loss_fr=494.102479\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:311\n",
      "loss_en=494.106914\n",
      "loss_fr=494.102479\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:312\n",
      "loss_en=494.106914\n",
      "loss_fr=494.102478\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:313\n",
      "loss_en=494.106914\n",
      "loss_fr=494.102478\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:314\n",
      "loss_en=494.106915\n",
      "loss_fr=494.102478\n",
      " fourniront fourniront fourniront fourniront fourniront\n",
      ">>Training...iteration:315\n",
      "loss_en=494.106914\n",
      "loss_fr=494.102478\n",
      " fourniront fourniront fourniront fourniront fourniront\n"
     ]
    }
   ],
   "source": [
    "while n!=1000:\n",
    "    curr_en = lines_en[p].split()\n",
    "    inputs_en = [word_to_ix_en[w] for w in curr_en[0:len(curr_en)-1]]\n",
    "    targets_en = [word_to_ix_en[w] for w in curr_en[1:len(curr_en)]]\n",
    "    curr_fr = lines_fr[p].split()\n",
    "    inputs_fr=[-1]\n",
    "    temp = [word_to_ix_fr[w] for w in curr_fr[0:len(curr_fr)-1]]\n",
    "    inputs_fr.extend(temp)\n",
    "    targets_fr = [word_to_ix_fr[w] for w in curr_fr[0:len(curr_fr)]]\n",
    "    if(len(inputs_en)<=5 or len(targets_en)<=5 or len(inputs_fr)<=5 or len(targets_fr)<=5):\n",
    "        p = p+1\n",
    "        continue\n",
    "    hprev = np.zeros((hidden_size,1))\n",
    "    cprev = np.zeros((hidden_size,1))\n",
    "#     print(\"\\bTraining Encoder\",n,p)\n",
    "    dWf_en, dWi_en, dWo_en,dWc_en,dWy_en,dbf_en,dbi_en,dbo_en,dbc_en,dby_en,hprev_en,cprev_en,loss_en = trainencoder(inputs_en,targets_en,hprev,cprev)\n",
    "#     print(\"\\bTraining Decoder\",n,p)\n",
    "    dWf_fr, dWi_fr, dWo_fr,dWc_fr,dWy_fr,dbf_fr,dbi_fr,dbo_fr,dbc_fr,dby_fr,hprev_fr,cprev_fr,loss_fr = traindecoder(inputs_fr,targets_fr, hprev_en,cprev_en)\n",
    "    p += 1 # move sentence pointer\n",
    "    if p >= num:\n",
    "        p = 0\n",
    "        np.savez(\"Weights/weights\"+str(n)+\".en\",wf_en=wf_en, wi_en = wi_en, wo_en = wo_en, wc_en = wc_en, wy_en = wy_en,bf_en = bf_en,bi_en = bi_en,bo_en = bo_en,bc_en = bc_en,by_en = by_en)\n",
    "        np.savez(\"Weights/weights\"+str(n)+\".fr\",wf_fr=wf_fr, wi_fr = wf_fr, wo_fr = wo_fr, wc_fr = wc_fr, wy_fr = wy_fr,bf_fr = bf_fr,bi_fr = bi_fr,bo_fr = bo_fr,bc_fr = bc_fr,by_fr = by_fr)\n",
    "        print('>>Training...iteration:%d'%(n))\n",
    "        print('loss_en=%f'%(loss_en))\n",
    "        print('loss_fr=%f'%(loss_fr))\n",
    "        sys.stdout.flush()\n",
    "        input_english = \"The agenda for the United\"\n",
    "        curr_en = input_english.split()\n",
    "        inputs_en = [word_to_ix_en[w] for w in curr_en[0:len(curr_en)-1]]\n",
    "        targets_en = [word_to_ix_en[w] for w in curr_en[1:len(curr_en)]]\n",
    "        output_words = test(inputs_en,targets_en)\n",
    "        print(output_words) \n",
    "        sys.stdout.flush()\n",
    "        n = n + 1\n",
    "        \n",
    "    for param_en, dparam_en, mem_en in zip([wf_en, wi_en, wo_en, wc_en, wy_en,bf_en,bi_en,bo_en,bc_en,by_en], \n",
    "                                [dWf_en, dWi_en, dWo_en, dWc_en, dWy_en,dbf_en,dbi_en,dbo_en,dbc_en,dby_en], \n",
    "                                [mdWf_en, mdWi_en, mdWo_en, mdWc_en, mdWy_en,mdbf_en,mdbi_en,mdbo_en,mdbc_en,mdby_en]):\n",
    "      mem_en += dparam_en * dparam_en\n",
    "      param_en += -learning_rate * dparam_en / np.sqrt(mem_en + 1e-8) # adagrad update\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for param_fr, dparam_fr, mem_fr in zip([wf_fr, wi_fr, wo_fr, wc_fr, wy_fr,bf_fr,bi_fr,bo_fr,bc_fr,by_fr], \n",
    "                                [dWf_fr, dWi_fr, dWo_fr, dWc_fr, dWy_fr,dbf_fr,dbi_fr,dbo_fr,dbc_fr,dby_fr], \n",
    "                                [mdWf_fr, mdWi_fr, mdWo_fr, mdWc_fr, mdWy_fr,mdbf_fr,mdbi_fr,mdbo_fr,mdbc_fr,mdby_fr]):\n",
    "      mem_fr += dparam_fr * dparam_fr\n",
    "      param_fr += -learning_rate * dparam_fr / np.sqrt(mem_fr + 1e-8) # adagrad update\n",
    " # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.savez(\"weights.en\",wxh_en=wxh_en,whh_en=whh_en,why_en=why_en,bh_en=bh_en,by_en=by_en)\n",
    "# np.savez(\"weights.fr\",wxh_fr=wxh_fr,whh_fr=whh_fr,why_fr=why_fr,bh_fr=bh_fr,by_fr=by_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
